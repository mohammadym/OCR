{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_43RlSAmSy9W",
        "outputId": "07ae6bab-c350-4c18-d18d-0337099becdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "# os.getcwd()\n",
        "os.chdir('/content/drive/My Drive/DeepLearning_FinalProject')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "\n",
        "def convert_output_to_text(output):\n",
        "  output_text = \"\"\n",
        "  for i in range(0, output.shape[0]):\n",
        "    for j in range(0, output.shape[1]):\n",
        "      index_word = np.argmax(output[i, j] == 1)\n",
        "      output_text += alphabet[index_word]\n",
        "  return output_text\n",
        "\n",
        "alphabet=\"0123456789 \"\n",
        "\n",
        "\n",
        "def predict(bank_or_melli, imgpath):\n",
        "  height = 32\n",
        "  if bank_or_melli == 1:\n",
        "    width = 222\n",
        "    transform_list =  [transforms.Grayscale(1),\n",
        "                          transforms.ToTensor(), \n",
        "                          transforms.Normalize((0.5,), (0.5,))]\n",
        "    transform = transforms.Compose(transform_list)                        \n",
        "    imB = Image.open(imgpath)\n",
        "    im = imB.resize((width, height)) \n",
        "    img = transform(im)\n",
        "    print(img.shape)\n",
        "    image = np.ones((1, height, width), dtype=np.float32)\n",
        "    print(image.shape)\n",
        "    image[:, :, 0:img.shape[2]] = img\n",
        "    # image = image.reshape((image.shape[1], image.shape[2], 1))\n",
        "    print(image.shape)\n",
        "    model = load_model('model_bank.h5')\n",
        "    # image = image.reshape((1, image.shape[0], image.shape[1]))\n",
        "    pred_y = model.predict(image)\n",
        "    pred_y = tf.one_hot(tf.argmax(pred_y, axis=2), pred_y.shape[2])\n",
        "    print(convert_output_to_text(pred_y))\n",
        "\n",
        "  if bank_or_melli == 2:\n",
        "    width = 200\n",
        "    transform_list =  [transforms.Grayscale(1),\n",
        "                          transforms.ToTensor(), \n",
        "                          transforms.Normalize((0.5,), (0.5,))]\n",
        "    transform = transforms.Compose(transform_list)                        \n",
        "    imB = Image.open(imgpath)\n",
        "    im = imB.resize((width, height)) \n",
        "    img = transform(im)\n",
        "    # print(img.shape)\n",
        "    image = np.ones((1, height, width), dtype=np.float32)\n",
        "    # print(image.shape)\n",
        "    image[:, :, 0:img.shape[2]] = img\n",
        "    # print(image.shape)\n",
        "    model = load_model('model_melli_2.h5')\n",
        "    pred_y = model.predict(image)\n",
        "    pred_y = tf.one_hot(tf.argmax(pred_y, axis=2), pred_y.shape[2])\n",
        "    print()\n",
        "    print(\"answer: \", convert_output_to_text(pred_y))"
      ],
      "metadata": {
        "id": "HnmPxndVTvNp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bank_or_melli = int(input(\"enter 1 if your card is a bank card or 2 if your card is a melli card: \"))\n",
        "file_name = input(\"enter your image file name: \")\n",
        "imgpath = \"/content/\" + file_name\n",
        "\n",
        "predict(bank_or_melli, imgpath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh_ZKSORXu_3",
        "outputId": "39a873c0-e9f9-4f20-94c4-4f867ffa3da8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter 1 if your card is a bank card or 2 if your card is a melli card: 1\n",
            "enter your image file name: a.png\n",
            "torch.Size([1, 32, 222])\n",
            "(1, 32, 222)\n",
            "(1, 32, 222)\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6d74930b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0283089 513333      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "POecUTypb7L6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}